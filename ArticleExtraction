import os
os.chdir(r'C:\Users\bhuva_pxpvpbh\Downloads\python-articlecrawler-master\python-articlecrawler-master')
import argparse
import json
import os
import uuid
import justext
import requests
import os
import pandas as pd
import re
import goose3
from bs4 import BeautifulSoup
from goose3 import Goose
#import clear_text
from clean_text import *
from trafilatura.utils import fetch_url
from trafilatura.utils import *
from goose3.utils import ReplaceSequence
from goose3.configuration import Configuration
from pdfminer.converter import PDFPageAggregator
from pdfminer.layout import LAParams, LTTextBoxHorizontal
from pdfminer.pdfdocument import PDFDocument
from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter
from pdfminer.pdfpage import PDFPage
from pdfminer.pdfparser import PDFParser
from requests.adapters import HTTPAdapter
from sqlalchemy import exists
from urllib3 import Retry
from collections import OrderedDict

corpus=pd.DataFrame()


BASE_DIR = os.path.dirname(r'C:\Users\bhuva_pxpvpbh\Downloads\python-articlecrawler-master\python-articlecrawler-master')

REQUESTS_TIMEOUT = 30
REQUESTS_RETRY = 3


class UnknownContentType(Exception):
    pass

class ExportArticle(object):
    """
    Export article to SQL or JSON (yet)

    """

    def __init__(self, article, to):
        """
        
        :param to: Extration type, only supports SQL and JSON
        :param article: ArticleCrawler object
        :return: None
        """

        # article must be an ArticleCrawler object
        assert isinstance(article, WEBSCRAPPING), "article must be an ArticleCrawler object"

        self.to = to
        self.article = article

        if not self.article.title and not self.article.content:
            return

        try:
            import sqlalchemy
        except ImportError:
            print("SqlAlchemy is not installed. Process will be extracted to JSON file")
            self.to = 'json'
        
        self.__extract_to_json() if self.to == 'json' else self.__extract_to_sql()
            
    def __extract_to_sql(self):
        """
        Creates article table if not exists
        If url already exists in database, it will check if html content (raw_content) has changed
        Otherwise it will create new article

        Database sets for SQLite3.
        #TODO: hardcoded to SQLite3, get parameter from user
        """

        from db import sql_session as sql

        is_exists = sql.query(exists().where(Article.url == self.article.url)).scalar()
        if is_exists:
            # TODO: redundant query count. is_exists should be combined with article variable. affects database performance.
            article = sql.query(Article).filter_by(url=self.article.url).first()
            if article.raw_content != self.article.raw_content:
                article.raw_content = self.article.raw_content
                article.content = self.article.content
                article.title = self.article.title
                article.meta_keywords = self.article.meta_keywords
                article.meta_description = self.article.meta_description
                article.images = json.dumps(self.article.images)
                sql.commit()
        else:
            article = Article(title=self.article.title,
                              content=self.article.content,
                              url=self.article.url,
                              raw_content=self.article.raw_content,
                              meta_description=self.article.meta_description,
                              meta_keywords=self.article.meta_keywords,
                              images=json.dumps(self.article.images))
            sql.add(article)
            sql.commit()

    def __extract_to_json(self):
        """
        Extracting data to JSON
        """
        print("inside json function")
        json_data = {}
        article_json_file = os.path.join(BASE_DIR, 'article.json')
        print(article_json_file)
        if os.path.exists(article_json_file):
            with open(article_json_file, 'r') as f:
                try:
                    json_data = json.load(f)
                except json.decoder.JSONDecodeError:
                    # It will delete JSON file in anyway
                    pass

            # delete json file so it can create it with edited version
            os.remove(article_json_file)
        else:
            json_data[self.article.url] = {}

        if url not in json_data:
            json_data[self.article.url] = {}

        json_data[self.article.url] = {'title': self.article.title,
                                       'content': self.article.content,
                                       'raw_content': self.article.raw_content,
                                       'url': self.article.url,
                                       'meta_keywords': self.article.meta_keywords,
                                       'meta_description': self.article.meta_description,
                                       'images': self.article.images}

        # create json file
        with open(article_json_file, 'w') as f:
            json.dump(json_data, f, indent=4)


class WEBSCRAPPING(object):
    """
        Getting article details
        Usage example:
            article = WEBSCRAPPING(url='some link')

            Example attributes:
                article.title = 'Example Title'
                article.content = 'Example Content'
                article.raw_content = returns full html body without parsing
                article.url = returns url for export to SQL or JSON
                article.images = returns list of images URLs
    """
    #article_dictionary={}

    def __init__(self, url):
        """
        Constructor of ArticleCrawler

        :param url: URL to fetch
        :return: None
        """
        self.url = url
        self.title = ''
        self.content = ''
        self.raw_content = ''
        self.images = []
        self.meta_description = ''
        self.meta_keywords = ''
        self.response = self.__get_response()

        if not self.response:
            return

        try:
            self.__get_content_type()
        except UnknownContentType:
            print()

        self.is_html = False
        self.is_pdf = False

    def __get_content_type(self):
        response = self.response
        content_type = response.headers['Content-Type']

        # unknown content types will be added to json file
        if 'text/html' in content_type:
            self.is_html = True
            self.get_article_details()
        elif content_type == 'application/pdf':
            self.is_pdf = True
            print("PDF rendering is still in progress")
        else:
            # if content type is not a expected type, it will write it to json file for
            # create textures folder if not exists
            path = os.path.join(BASE_DIR, 'textures')
            if not os.path.exists(path):
                os.makedirs(path)
            json_data = {}
            # read json file first if exists
            unknown_content_types_file = os.path.join(path, 'unknown-content-types.json')
            if os.path.exists(unknown_content_types_file):
                with open(unknown_content_types_file, 'r') as f:
                    try:
                        json_data = json.load(f)
                    except json.decoder.JSONDecodeError:
                        # It will delete JSON file in anyway
                        pass

                # delete json file so it can create it with edited version
                os.remove(unknown_content_types_file)
            else:
                json_data['content_types'] = []

            # for broken JSON files, there must be content_types key in dict
            if 'content_types' not in json_data:
                json_data['content_types'] = []

            json_data['content_types'].append(content_type)

            # create json file
            with open(unknown_content_types_file, 'w') as f:
                json.dump(json_data, f, indent=50)

            raise UnknownContentType
        
    def __get_response(self):
        with requests.Session() as s:
            retry = Retry(total=REQUESTS_RETRY, backoff_factor=0.3, status_forcelist=[500, 503])
            adapter = HTTPAdapter(max_retries=retry)
            s.mount('https://', adapter=adapter)
            s.mount('http://', adapter=adapter)
            try:
                return s.get(self.url, timeout=REQUESTS_TIMEOUT)
            except (requests.exceptions.ReadTimeout, requests.exceptions.ConnectionError):
                # TODO: Do something in case of connectionerror and/or readtimeout
                return
            except (requests.exceptions.MissingSchema):
                print("Invalid URL, Please make sure to add http:// or https:// to URL")

    def __process_goose(self):
        goose_config = Configuration()
        goose_config.browser_user_agent = 'Mozilla'
        goose_config.enable_image_fetching = True
        g = Goose(config=goose_config)
        try:
   
            article_images = g.extract(self.url)

            if article_images.top_image.src:
                self.images = self.get_all_images_from_example_src(article_images.top_image.src)

        except (requests.exceptions.ConnectionError, requests.exceptions.ReadTimeout):
            return None



    def get_article_details(self):
        
        goose_object = self.__process_goose()
        title = None
        content = None
        downloaded = fetch_url(self.url)
        content =extract(downloaded,include_formatting=True)
        if goose_object:
            title = goose_object.title
            # Removing newlines and tabs in article content
            content = cleaned_text.replace('\n', '\t') if goose_object.cleaned_text else None
        # If Goose can not found title or content, will try jusText to get article
        if not title or not content:
            content_language = None
            for key, value in self.response.headers.items():
                if "language" in key.lower():
                    content_language = value    
            # If not content language has found, English will be default language
            if not content_language:
                parapraphs = justext.justext(self.response.content,justext.get_stoplist(language='English'))
            else:
                path = os.path.join(BASE_DIR, 'textures')
                if not os.path.exists(path):
                    os.makedirs(path)

                # read json file first if exists
                language_codes_json = os.path.join(path, 'language_codes.json')
                stoplist_language = "English"

                if os.path.exists(language_codes_json):
                    with open(language_codes_json, 'r') as f:
                        language_data = json.load(f)

                    for key, value in language_data.items():
                        if key == content_language:
                            stoplist_language = value
                parapraphs = justext.justext(content.replace('\n', '\t') , justext.get_stoplist(language=stoplist_language))

            # Goose would have found title in article
            if not title:
                try:
                    title = [parapraph.text for parapraph in parapraphs if not parapraph.is_boilerplate and parapraph.is_heading and parapraph.class_type == 'good'][0]
                except IndexError:
                    pass
                
        self.title = title
        self.content = content
        self.raw_content = self.response.text

if __name__ == '__main__':
    
    data_useful=[]
    url='Provide your URL for fetching article data'
    argparser = argparse.ArgumentParser(description='Article crawler')
    argparser.add_argument('--url', help=url,default=url)
    argparser.add_argument('--export', help='json', default='json')
    parser = argparser.parse_args()
    url = parser.url
    export_option = parser.export
    article= WEBSCRAPPING(url=url)
    ExportArticle(article=article, to=export_option)
    data=[article.title]
    print(article.title)
    (data.append(article.content))
    data_useful.extend(data)
        

    
        
        
    
    
    
 
   
    
    
